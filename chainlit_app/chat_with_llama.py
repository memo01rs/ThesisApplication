import chainlit as cl
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain.callbacks.base import BaseCallbackHandler


class StreamHandler(BaseCallbackHandler):
    """
    A custom callback handler for streaming responses in real-time.

    This handler sends tokens as they are generated by the LLM, allowing
    for a streaming response experience in the chat interface.
    """

    def __init__(self):
        self.msg = cl.Message(content="")

    async def on_llm_new_token(self, token: str, **kwargs):
        """
        Handles new tokens generated by the LLM during the response generation.

        Args:
            token (str): The newly generated token by the LLM.
        """
        await self.msg.stream_token(token)

    async def on_llm_end(self, response: str, **kwargs):
        """
        Handles the end of the LLM's response generation.

        Args:
            response (str): The final response generated by the LLM.
        """
        await self.msg.send()
        self.msg = cl.Message(content="")  # Reset message for the next response


# Define the prompt template for the LLM
template = """
You are an AI assistant specialized in analyzing and interpreting regulations and legal texts.
You provide comprehensive, accurate and factual responses based on the given instructions.
Your answers should be detailed, professional, and relevant to the field of regulatory analysis.

Always provide a concise answer and use the following context/conversation history: {context}

Question: {question}

Answer: 
"""

# Initialize the LLM and set up the chain
model = OllamaLLM(model="llama3")
prompt = ChatPromptTemplate.from_template(template)
memory = ConversationBufferMemory(memory_key="context")
global_chain = LLMChain(prompt=prompt, llm=model, memory=memory)


@cl.on_chat_start
def on_chat_start():
    """
    Initializes the chat session and sets up the LLM chain.

    This function is called when a chat session starts. It stores the chain
    in the user session for later use.
    """
    cl.user_session.set("chain", global_chain)


@cl.on_message
async def on_message(message: cl.Message):
    """
    Processes incoming messages and generates a response using the LLM chain.

    Args:
        message (cl.Message): The incoming message from the user.
    """
    chain_instance = cl.user_session.get("chain")

    # Generate the response by invoking the chain
    result = await chain_instance.ainvoke(
        {
            "context": chain_instance.memory.load_memory_variables({})["context"],
            "question": message.content
        },
        config={"callbacks": [cl.AsyncLangchainCallbackHandler(), StreamHandler()]}
    )
    await message.send()


if __name__ == "__main__":
    cl.run()
