import logging
import chainlit as cl
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain_community.llms import CTransformers
from langchain_core.prompts import PromptTemplate

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class StreamHandler(BaseCallbackHandler):
    """
    Custom callback handler for streaming responses in real-time.

    This handler sends tokens as they are generated by the LLM, allowing
    for a streaming response experience in the chat interface.
    """

    def __init__(self):
        self.msg = cl.Message(content="")
        logger.debug("StreamHandler initialized")

    async def on_llm_new_token(self, token: str, **kwargs):
        """
        Handle the event of a new token being generated by the LLM.

        Args:
            token (str): The newly generated token by the LLM.
        """
        logger.debug(f"New token received: {token}")
        await self.msg.stream_token(token)

    async def on_llm_end(self, response: str, **kwargs):
        """
        Handle the end of the LLM's response.

        Args:
            response (str): The final response generated by the LLM.
        """
        logger.debug("LLM response ended")
        await self.msg.send()
        self.msg = cl.Message(content="")  # Reset message for next response


# Initialize the LLM with the specified configuration
llm = CTransformers(
    model="/Users/mehmetarslan/PycharmProjects/ThesisApplication/models",
    model_file="llama-2-7b-chat.Q6_K.gguf",
    model_type="llama",
    max_new_tokens=2048,
    context_length=4096
)
logger.debug("LLM initialized")

# Define the prompt template
template = """
You are an AI assistant specialized in analyzing and interpreting regulations and legal texts.
You provide comprehensive, accurate and factual responses based on the given instructions.
Your answers should be detailed, professional, and relevant to the field of regulatory analysis.

Always provide a concise answer and use the following Context:
{context}

User:
{instruction}"""

prompt = PromptTemplate(template=template, input_variables=["context", "instruction"])
logger.debug("Prompt template created")


@cl.on_chat_start
def on_chat_start():
    """
    Initializes the chat session and sets up the LLM chain with memory.

    This function is called when a chat session starts. It sets up
    the LLMChain with a conversation buffer to maintain context across
    interactions.
    """
    memory = ConversationBufferMemory(memory_key="context")
    llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=False, memory=memory)
    cl.user_session.set("llm_chain", llm_chain)
    logger.debug("Chat started and LLMChain set")


@cl.on_message
async def on_message(message: cl.Message):
    """
    Processes incoming messages and generates a response using the LLM chain.

    Args:
        message (cl.Message): The incoming message from the user.
    """
    llm_chain = cl.user_session.get("llm_chain")
    logger.debug(f"Message received: {message.content}")

    await llm_chain.ainvoke(
        message.content,
        config={"callbacks": [cl.AsyncLangchainCallbackHandler(), StreamHandler()]},
    )
    logger.debug("Message processed")
